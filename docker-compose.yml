services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    # Ensure LiteLLM runs in a way that it can map requests
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--detailed_debug" ]

  router:
    build: ./claude-code-router
    ports:
      - "3000:3000"
    volumes:
      - ./claude-code-router/config.json:/root/.claude-code-router/config.json
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    # Logs will be written to /root/.claude-code-router/logs inside the container by the app

  claude-code:
    build: .
    volumes:
      - .:/app
      - claude-config:/root/.claude
      - claude-config-local:/root/.anthropic
    environment:
      # Point Claude Code to the LiteLLM proxy
      # Note: This assumes LiteLLM can handle the Anthropic format requests that Claude Code sends.
      - ANTHROPIC_BASE_URL=${ANTHROPIC_BASE_URL}
      # Set a dummy token to satisfy Claude Code's auth requirement and LiteLLM's check
      - ANTHROPIC_AUTH_TOKEN=sk-dummy-key
    depends_on:
      - litellm
    # Interactive mode settings
    tty: true
    stdin_open: true

volumes:
  claude-config:
  claude-config-local:
